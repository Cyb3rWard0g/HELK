{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction to Spark SQL via PySpark**\n",
    "----------------------------------------------------------------------------\n",
    "## Goals:\n",
    "* Get familiarized with the basics of Spark SQL and PySpark\n",
    "* Learn to create a SparkSession\n",
    "* Verify if Jupyter can talk to Spark Master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "* https://spark.apache.org/docs/latest/api/python/pyspark.html\n",
    "* https://spark.apache.org/docs/latest/sql-getting-started.html\n",
    "* https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession\n",
    "* https://jaceklaskowski.gitbooks.io/mastering-spark-sql/\n",
    "* http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Spark SQL?\n",
    "* It is a Spark module that leverages Sparks functional programming APIs to allow SQL relational processing tasks on (semi)structured data.\n",
    "* Spark SQL provides Spark with more information about the structure of both the data and the computation being performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PySpark?\n",
    "PySpark is the Python API for Spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do I start using Pyspark and SparkSQL?\n",
    "* Start by importing the PySpark SQL SparkSession Class and creating a SparkSession instance .\n",
    "* A SparkSession class is considered the entry point to programming Spark with the Dataset and DataFrame API.\n",
    "* A SparkSession can be used create DataFrames, register DataFrames as tables, execute SQL over tables, cache tables, and read parquet files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import SparkSession Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a SparkSession?\n",
    "* It is the driver process that controls a spark application\n",
    "* A SparkSession instance is responsible for executing the driver program’s commands (code) across executors (in a cluster) to complete a given task.\n",
    "* You can have as many SparkSessions as you want in a single Spark application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do I create a SparkSession?\n",
    "* You can use the SparkSession class attribute called **[Builder](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession.builder)**.\n",
    "* The class attribute builder allows you to run some of the following functions:\n",
    "    * **appName**: Sets a name for the application\n",
    "    * **master**: URL for the Spark master (Local or Spark standalone cluster)\n",
    "    * **enableHiveSupport**: Enables Hive support, including connectivity to a persistent Hive metastore, support for Hive serdes, and Hive user-defined functions.\n",
    "    * **getOrCreate**:Gets an existing SparkSession or, if there is no existing one, creates a new one based on the options set in this builder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a SparkSession instance\n",
    "* Define a **spark** variable\n",
    "* Pass values to the **appName** and **master** functions\n",
    "    * For the master function, we are going to use the HELK's Spark Master container (helk-spark-master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .master(\"spark://helk-spark-master:7077\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the SparkSession variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://403892d82956:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://helk-spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL basic example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa015e94f28>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Dataframe?\n",
    "* In Spark, a dataframe is the most common Structured API, and it is used to represent data in a table format with rows and columns.\n",
    "* Think of a dataframe as a spreadsheet with headers. The difference is that one Spark Dataframe can be distributed across several computers due to its large size or high computation requirements for faster analysis.\n",
    "* The list of column names from a dataframe with its respective data types is called the **schema**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is a Spark Dataframe the same as a Python Pandas Dataframe?\n",
    "* A Python dataframe sits on one computer whereas a Spark Dataframe, once again, can be distributed across several computers.\n",
    "* PySpark allows the conversion from Python Pandas dataframes to Spark dataframes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your first Dataframe\n",
    "Let's create our first dataframe by using **range** and **toDF** functions.\n",
    "* One column named **numbers**\n",
    "* 10 rows containing numbers from 0-9\n",
    "\n",
    "**[range(start, end=None, step=1, numPartitions=None)](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession.range)**\n",
    "* Create a DataFrame with single pyspark.sql.types.LongType column named id, containing elements in a range from start to end (exclusive) with step value step.\n",
    "\n",
    "**[toDF(*cols)](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.toDF)**\n",
    "* Returns a new class:DataFrame that with new specified column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_df = spark.range(10).toDF(\"numbers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|numbers|\n",
      "+-------+\n",
      "|      0|\n",
      "|      1|\n",
      "|      2|\n",
      "|      3|\n",
      "|      4|\n",
      "|      5|\n",
      "|      6|\n",
      "|      7|\n",
      "|      8|\n",
      "|      9|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create another Dataframe\n",
    "**[createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession.createDataFrame)**\n",
    "\n",
    "* Creates a DataFrame from an RDD, a list or a pandas.DataFrame.\n",
    "* When schema is a list of column names, the type of each column will be inferred from data.\n",
    "* When schema is None, it will try to infer the schema (column names and types) from data, which should be an RDD of Row, or namedtuple, or dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_data=[['Pedro','Doberman',3],['Clementine','Golden Retriever',8],['Norah','Great Dane',6]\\\n",
    "         ,['Mabel','Austrailian Shepherd',1],['Bear','Maltese',4],['Bill','Great Dane',10]]\n",
    "dog_df=spark.createDataFrame(dog_data, ['name','breed','age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---+\n",
      "|      name|               breed|age|\n",
      "+----------+--------------------+---+\n",
      "|     Pedro|            Doberman|  3|\n",
      "|Clementine|    Golden Retriever|  8|\n",
      "|     Norah|          Great Dane|  6|\n",
      "|     Mabel|Austrailian Shepherd|  1|\n",
      "|      Bear|             Maltese|  4|\n",
      "|      Bill|          Great Dane| 10|\n",
      "+----------+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dog_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the Dataframe schema\n",
    "* We are going to do apply a concept called schema inference which lets spark takes its best guess at figuring out the schema.\n",
    "* Spark reads part of the dataframe and then tries to parse the types of data in each row. \n",
    "* You can also define a strict schema when you read in data which does not let Spark guess. This is recommended for production use cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[schema](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.schema)**\n",
    "* Returns the schema of this DataFrame as a pyspark.sql.types.StructType."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(breed,StringType,true),StructField(age,LongType,true)))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[printSchema()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.printSchema)**\n",
    "* Prints out the schema in the tree format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- breed: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dog_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Dataframe Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[select(*cols)](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.select)**\n",
    "* Projects a set of expressions and returns a new DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access Dataframes's columns by attribute (df.name):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      name|\n",
      "+----------+\n",
      "|     Pedro|\n",
      "|Clementine|\n",
      "|     Norah|\n",
      "|     Mabel|\n",
      "|      Bear|\n",
      "|      Bill|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dog_df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access Dataframe's columns by indexing (df['name']). \n",
    "* According to Sparks documentation, the indexing form is the recommended one because it is future proof and won’t break with column names that are also attributes on the DataFrame class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      name|\n",
      "+----------+\n",
      "|     Pedro|\n",
      "|Clementine|\n",
      "|     Norah|\n",
      "|     Mabel|\n",
      "|      Bear|\n",
      "|      Bill|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dog_df.select(dog_df[\"name\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[filter(condition)](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.filter)**\n",
    "* Filters rows using the given condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select dogs that are older than 4 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+---+\n",
      "|      name|           breed|age|\n",
      "+----------+----------------+---+\n",
      "|Clementine|Golden Retriever|  8|\n",
      "|     Norah|      Great Dane|  6|\n",
      "|      Bill|      Great Dane| 10|\n",
      "+----------+----------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dog_df.filter(dog_df[\"age\"] > 4).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[groupBy(*cols)](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy)**\n",
    "* Groups the DataFrame using the specified columns, so we can run aggregation on them. See GroupedData for all the available aggregate functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "group dogs and count them by their age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "|  6|    1|\n",
      "|  1|    1|\n",
      "| 10|    1|\n",
      "|  3|    1|\n",
      "|  8|    1|\n",
      "|  4|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dog_df.groupBy(dog_df[\"age\"]).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SQL queries on your Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[createOrReplaceTempView(name)](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.createOrReplaceTempView)**\n",
    "* Creates or replaces a local temporary view with this DataFrame.\n",
    "* The lifetime of this temporary table is tied to the SparkSession that was used to create this DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the current Dataframe as a SQL temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---+\n",
      "|      name|               breed|age|\n",
      "+----------+--------------------+---+\n",
      "|     Pedro|            Doberman|  3|\n",
      "|Clementine|    Golden Retriever|  8|\n",
      "|     Norah|          Great Dane|  6|\n",
      "|     Mabel|Austrailian Shepherd|  1|\n",
      "|      Bear|             Maltese|  4|\n",
      "|      Bill|          Great Dane| 10|\n",
      "+----------+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dog_df.createOrReplaceTempView(\"dogs\")\n",
    "\n",
    "sql_dog_df = spark.sql(\"SELECT * FROM dogs\")\n",
    "sql_dog_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+\n",
      "| name|   breed|age|\n",
      "+-----+--------+---+\n",
      "|Pedro|Doberman|  3|\n",
      "+-----+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_dog_df = spark.sql(\"SELECT * FROM dogs WHERE name='Pedro'\")\n",
    "sql_dog_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark_Python3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
