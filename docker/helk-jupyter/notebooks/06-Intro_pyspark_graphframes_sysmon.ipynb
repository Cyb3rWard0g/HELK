{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark Graphframes and Sysmon\n",
    "----------------------------------------------------------------------------\n",
    "## Goals:\n",
    "* Get familiarized with the basics of Spark Graphframes\n",
    "* Confirm Jupyter can import Graphframes library\n",
    "* Confirm Spark can read data from Elasticsearch\n",
    "* Learn to create a graphframe from sysmon Index\n",
    "* Learn the basics of GraphFrames Motifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Graphframes?\n",
    "GraphFrames is a package for Apache Spark which provides DataFrame-based Graphs. \n",
    "* It provides high-level APIs in Scala, Java, and Python. \n",
    "* It aims to provide both the functionality of GraphX and extended functionality taking advantage of Spark DataFrames.\n",
    "* This extended functionality includes motif finding, DataFrame-based serialization, and highly expressive graph queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a graph?\n",
    "GraphFrames represent graphs: \n",
    "* Vertices (e.g., users)\n",
    "* Edges (e.g., relationships between users)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import SparkSession Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a SparkSession instance\n",
    "* Define a **spark** variable\n",
    "* Pass values to the **appName** and **master** functions\n",
    "    * For the master function, we are going to use the HELK's Spark Master container (helk-spark-master)\n",
    "* This time add the **config()** function to set Elasticsearch information needed to read from it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**config(key=None, value=None, conf=None)**](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession.Builder.config)\n",
    "* Sets a config option.\n",
    "* Options set using this method are automatically propagated to both SparkConf and SparkSessionâ€™s own configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HELK Graphs\") \\\n",
    "    .master(\"spark://helk-spark-master:7077\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the SparkSession variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c1fb2000cc99:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://helk-spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>HELK Graphs</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4da80794e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Graphframes & SQL Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|inDegree|\n",
      "+---+--------+\n",
      "|  c|       1|\n",
      "|  b|       2|\n",
      "+---+--------+\n",
      "\n",
      "CPU times: user 348 ms, sys: 380 ms, total: 728 ms\n",
      "Wall time: 31.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create a Vertex DataFrame with unique ID column \"id\"# Creat \n",
    "v = spark.createDataFrame([\n",
    "  (\"a\", \"Alice\", 34),\n",
    "  (\"b\", \"Bob\", 36),\n",
    "  (\"c\", \"Charlie\", 30),\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "# Create an Edge DataFrame with \"src\" and \"dst\" columns\n",
    "e = spark.createDataFrame([\n",
    "  (\"a\", \"b\", \"friend\"),\n",
    "  (\"b\", \"c\", \"follow\"),\n",
    "  (\"c\", \"b\", \"follow\"),\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "# Create a GraphFrame\n",
    "from graphframes import *\n",
    "g = GraphFrame(v, e)\n",
    "\n",
    "# Query: Get in-degree of each vertex.\n",
    "g.inDegrees.show()\n",
    "\n",
    "# Query: Count the number of \"follow\" connections in the graph.\n",
    "g.edges.filter(\"relationship = 'follow'\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from the HELK Elasticsearch via Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Dataframe API to access Elasticsearch index (Elasticsearch-Sysmon Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we know, Spark SQL is a Spark module for structured data processing, and provides a programming abstraction called DataFrames and can also act as distributed SQL query engine.\n",
    "* Elasticsearch becomes a native source for Spark SQL so that data can be indexed and queried from Spark SQL transparently\n",
    "* Spark SQL works with structured data - in other words, all entries are expected to have the same structure (same number of fields, of the same type and name)\n",
    "* Using unstructured data (documents with different structures) is not supported and will cause problems.\n",
    "* Through the **org.elasticsearch.spark.sql** package, esDF methods are available on the SQLContext API\n",
    "\n",
    "Reference: https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_reader = (spark.read\n",
    "    .format(\"org.elasticsearch.spark.sql\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"es.read.field.as.array.include\", \"tags\")\n",
    "    .option(\"es.nodes\",\"helk-elasticsearch:9200\")\n",
    "    .option(\"es.net.http.auth.user\",\"elastic\")\n",
    ")\n",
    "    #PLEASE REMEMBER!!!!\n",
    "    #If you are using elastic TRIAL license, then you need the es.net.http.auth.pass config option set\n",
    "    #Example: .option(\"es.net.http.auth.pass\",\"elasticpassword\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**load(path=None, format=None, schema=None, **options)**](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.load)\n",
    "* Loads data from a data source and returns it as a :class`DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 1.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sysmon_df = es_reader.load(\"logs-endpoint-winevent-sysmon-*/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProcessCreate & Motifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vertices Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to replace the column name from **process_guid** to **id** because thats the column name that Graphframes uses for the row IDs.\n",
    "\n",
    "[**withColumn(colName, col)**](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join)\n",
    "* Returns a new DataFrame by adding a column or replacing the existing column that has the same name.\n",
    "* The column expression must be an expression over this DataFrame; attempting to add a column from some other dataframe will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = (sysmon_df.withColumn(\"id\", sysmon_df.process_guid)\n",
    "            .select(\"id\",\"user_name\",\"host_name\",\"process_parent_name\",\"process_name\",\"action\")\n",
    "           )\n",
    "vertices = vertices.filter(vertices.action == \"processcreate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+---------------+---------------+-------------------+----------------------+-------------+\n",
      "|id                                  |user_name      |host_name      |process_parent_name|process_name          |action       |\n",
      "+------------------------------------+---------------+---------------+-------------------+----------------------+-------------+\n",
      "|aa6b4a20-7cd9-5ce0-0000-0010a3801e00|pgustavo       |it001.shire.com|svchost.exe        |backgroundtaskhost.exe|processcreate|\n",
      "|aa6b4a20-7cdf-5ce0-0000-00105eac1e00|pgustavo       |it001.shire.com|powershell.exe     |conhost.exe           |processcreate|\n",
      "|aa6b4a20-7d15-5ce0-0000-0010f07f1f00|pgustavo       |it001.shire.com|svchost.exe        |backgroundtaskhost.exe|processcreate|\n",
      "|aa6b4a20-7d16-5ce0-0000-001089921f00|pgustavo       |it001.shire.com|svchost.exe        |runtimebroker.exe     |processcreate|\n",
      "|03ba39f5-7d20-5ce0-0000-001052da2002|network service|hr001.shire.com|svchost.exe        |gpupdate.exe          |processcreate|\n",
      "+------------------------------------+---------------+---------------+-------------------+----------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 1.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vertices.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Edges Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to make sure we also rename our **process_parent_guid** to **src** and **process_guid** to **dst**. This is to look for that relationship across our whole environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**selectExpr(*expr)**](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.selectExpr)\n",
    "* Projects a set of SQL expressions and returns a new DataFrame.\n",
    "* This is a variant of select() that accepts SQL expressions.\n",
    "* You can also combine selecting columns and renaming columns in a single step with selectExpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = (sysmon_df\n",
    "         .filter(sysmon_df.action == \"processcreate\")\n",
    "         .selectExpr(\"process_parent_guid as src\",\"process_guid as dst\")\n",
    "         .withColumn(\"relationship\", lit(\"spawned\"))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------------------------------------+------------+\n",
      "|src                                 |dst                                 |relationship|\n",
      "+------------------------------------+------------------------------------+------------+\n",
      "|aa6b4a20-7719-5ce0-0000-001068a30000|aa6b4a20-7cd9-5ce0-0000-0010a3801e00|spawned     |\n",
      "|aa6b4a20-7cde-5ce0-0000-00109ea71e00|aa6b4a20-7cdf-5ce0-0000-00105eac1e00|spawned     |\n",
      "|aa6b4a20-7719-5ce0-0000-001068a30000|aa6b4a20-7d15-5ce0-0000-0010f07f1f00|spawned     |\n",
      "|aa6b4a20-7719-5ce0-0000-001068a30000|aa6b4a20-7d16-5ce0-0000-001089921f00|spawned     |\n",
      "|03ba39f5-ea64-5ccb-0000-0010c91c0100|03ba39f5-7d20-5ce0-0000-001052da2002|spawned     |\n",
      "+------------------------------------+------------------------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 453 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "edges.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Graph (Vertices & Edges DataFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = GraphFrame(vertices, edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process A spawning Process B AND Process B Spawning Process C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 405 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "motifs = g.find(\"(a)-[]->(b);(b)-[]->(c)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+--------------+------------+\n",
      "|process_parent_name|process_name|process_name  |process_name|\n",
      "+-------------------+------------+--------------+------------+\n",
      "|svchost.exe        |wmiprvse.exe|powershell.exe|conhost.exe |\n",
      "|svchost.exe        |wmiprvse.exe|powershell.exe|whoami.exe  |\n",
      "|explorer.exe       |wscript.exe |powershell.exe|conhost.exe |\n",
      "+-------------------+------------+--------------+------------+\n",
      "\n",
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(motifs\n",
    "     .select(\"a.process_parent_name\",\"a.process_name\",\"b.process_name\",\"c.process_name\")\n",
    "     .show(20,truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|process_parent_name|count|\n",
      "+-------------------+-----+\n",
      "|       explorer.exe|    1|\n",
      "|        svchost.exe|    2|\n",
      "+-------------------+-----+\n",
      "\n",
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 13.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "motifs.groupby('a.process_parent_name').count().sort('count').show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark_Python3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
